{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0WViBS4E7yh1"
   },
   "source": [
    "# Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S1iZkq3j9PWJ"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from joblib import dump, load\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import joblib\n",
    "from skimage import io, color, filters\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_regression, f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "snzl_-7u9kBA"
   },
   "outputs": [],
   "source": [
    "# Grid-based dataset csv file\n",
    "DATASET_CSV_PATH = '../dataset-numpy'\n",
    "\n",
    "# Scaler location\n",
    "SCALER_PATH = '../classifiers/scaler.joblib'\n",
    "\n",
    "# KNN Classifier location\n",
    "KNN_CLASSIFIER_PATH = '../classifiers/knn_classifier.joblib'\n",
    "\n",
    "# Temp directory\n",
    "TEMP_DIR_PATH = '../temp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GnJ-aDthWGXC"
   },
   "source": [
    "# Grid Based Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DdJf-ypXX5EC"
   },
   "source": [
    "It's always wise to check that we actually have the data we expect. After importing the datasets we created previously from Github, we should have the data for every grid size. Given the initial dataset, we should have 1920 rows (as it consists of 480 postal codes, which is 1920 single digits) for each grid size.\n",
    "\n",
    "Let's load a dataset from the csv file into a DataFrame and describe it, to see if we indeed retrieved everything in properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 711,
     "status": "ok",
     "timestamp": 1586380213554,
     "user": {
      "displayName": "Jasper Sijgers",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgMV5mgyq6OiBFr7_IG1qHskr3VehldL5sVO79W=s64",
      "userId": "09274429075529465880"
     },
     "user_tz": -120
    },
    "id": "HGRsNkwHCXjK",
    "outputId": "488b140c-a9aa-4d9d-b0d4-6205bd64db63"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sum_1</th>\n",
       "      <th>sum_2</th>\n",
       "      <th>sum_3</th>\n",
       "      <th>sum_4</th>\n",
       "      <th>sum_5</th>\n",
       "      <th>sum_6</th>\n",
       "      <th>sum_7</th>\n",
       "      <th>sum_8</th>\n",
       "      <th>sum_9</th>\n",
       "      <th>sum_10</th>\n",
       "      <th>sum_11</th>\n",
       "      <th>sum_12</th>\n",
       "      <th>sum_13</th>\n",
       "      <th>sum_14</th>\n",
       "      <th>sum_15</th>\n",
       "      <th>sum_16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>1920.000000</td>\n",
       "      <td>1920.000000</td>\n",
       "      <td>1920.000000</td>\n",
       "      <td>1920.000000</td>\n",
       "      <td>1920.000000</td>\n",
       "      <td>1920.000000</td>\n",
       "      <td>1920.000000</td>\n",
       "      <td>1920.000000</td>\n",
       "      <td>1920.000000</td>\n",
       "      <td>1920.000000</td>\n",
       "      <td>1920.000000</td>\n",
       "      <td>1920.000000</td>\n",
       "      <td>1920.000000</td>\n",
       "      <td>1920.000000</td>\n",
       "      <td>1920.000000</td>\n",
       "      <td>1920.000000</td>\n",
       "      <td>1920.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>4.532813</td>\n",
       "      <td>2.235937</td>\n",
       "      <td>39.644792</td>\n",
       "      <td>36.097396</td>\n",
       "      <td>3.843750</td>\n",
       "      <td>4.896354</td>\n",
       "      <td>34.896875</td>\n",
       "      <td>33.200521</td>\n",
       "      <td>4.411458</td>\n",
       "      <td>3.408854</td>\n",
       "      <td>30.311979</td>\n",
       "      <td>36.148438</td>\n",
       "      <td>6.444792</td>\n",
       "      <td>1.163542</td>\n",
       "      <td>35.611979</td>\n",
       "      <td>37.214583</td>\n",
       "      <td>6.365625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>2.868122</td>\n",
       "      <td>3.517256</td>\n",
       "      <td>12.735939</td>\n",
       "      <td>14.626580</td>\n",
       "      <td>7.979361</td>\n",
       "      <td>5.633055</td>\n",
       "      <td>15.674676</td>\n",
       "      <td>17.059070</td>\n",
       "      <td>6.696306</td>\n",
       "      <td>5.611908</td>\n",
       "      <td>18.605251</td>\n",
       "      <td>14.065148</td>\n",
       "      <td>6.858434</td>\n",
       "      <td>2.631299</td>\n",
       "      <td>13.570497</td>\n",
       "      <td>15.269706</td>\n",
       "      <td>9.307086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>46.250000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             label        sum_1        sum_2        sum_3        sum_4  \\\n",
       "count  1920.000000  1920.000000  1920.000000  1920.000000  1920.000000   \n",
       "mean      4.532813     2.235937    39.644792    36.097396     3.843750   \n",
       "std       2.868122     3.517256    12.735939    14.626580     7.979361   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       2.000000     0.000000    34.000000    27.000000     0.000000   \n",
       "50%       5.000000     0.000000    42.000000    38.000000     0.000000   \n",
       "75%       7.000000     3.000000    49.000000    47.000000     4.000000   \n",
       "max       9.000000    21.000000    63.000000    64.000000    53.000000   \n",
       "\n",
       "             sum_5        sum_6        sum_7        sum_8        sum_9  \\\n",
       "count  1920.000000  1920.000000  1920.000000  1920.000000  1920.000000   \n",
       "mean      4.896354    34.896875    33.200521     4.411458     3.408854   \n",
       "std       5.633055    15.674676    17.059070     6.696306     5.611908   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000    23.000000    21.000000     0.000000     0.000000   \n",
       "50%       3.000000    38.000000    36.000000     0.000000     0.000000   \n",
       "75%       8.000000    47.000000    46.000000     7.000000     5.000000   \n",
       "max      30.000000    64.000000    64.000000    36.000000    39.000000   \n",
       "\n",
       "            sum_10       sum_11       sum_12       sum_13       sum_14  \\\n",
       "count  1920.000000  1920.000000  1920.000000  1920.000000  1920.000000   \n",
       "mean     30.311979    36.148438     6.444792     1.163542    35.611979   \n",
       "std      18.605251    14.065148     6.858434     2.631299    13.570497   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%      15.000000    27.000000     0.000000     0.000000    29.000000   \n",
       "50%      31.000000    37.000000     5.000000     0.000000    37.000000   \n",
       "75%      46.250000    46.000000    11.000000     1.000000    45.000000   \n",
       "max      64.000000    64.000000    31.000000    28.000000    64.000000   \n",
       "\n",
       "            sum_15       sum_16  \n",
       "count  1920.000000  1920.000000  \n",
       "mean     37.214583     6.365625  \n",
       "std      15.269706     9.307086  \n",
       "min       0.000000     0.000000  \n",
       "25%      31.000000     0.000000  \n",
       "50%      41.000000     1.000000  \n",
       "75%      47.000000    10.000000  \n",
       "max      64.000000    50.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the dataset into a DataFrame\n",
    "df_grid = pd.read_csv(DATASET_CSV_PATH + '/grid_dataset_4.csv')\n",
    "\n",
    "# Describe the dataset to see if it's been loaded in properly\n",
    "df_grid.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iuohCXFSTgrM"
   },
   "source": [
    "## Dataset Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gpgLub-hY86V"
   },
   "source": [
    "To score a dataset, we're going to need to create a model (such as a KNN (K Nearest Neighbors) classifier) to train, and then see how good it does on the data we give it. A simple function should do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G0EiU8WTZ6gH"
   },
   "outputs": [],
   "source": [
    "def score_model(dataset):\n",
    "  # Get the dataset without the labels from the DataFrame\n",
    "  X = dataset.drop('label', axis=1)\n",
    "\n",
    "  # Get an array of labels that correspond with the dataset above\n",
    "  Y = dataset['label']\n",
    "\n",
    "  # Create a MinMaxScaler\n",
    "  scaler = MinMaxScaler()\n",
    "\n",
    "  # Fit the scaler to the dataset and scale the data\n",
    "  X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "  # Create train and test sets (for both data and labels), with 90% of the data\n",
    "  # being used to train and 10% to test the model\n",
    "  X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=.1)\n",
    "\n",
    "  # Create a KNN Classifier with K = 3\n",
    "  knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "  # Fit the classifier on the training set\n",
    "  knn_classifier.fit(X_train, Y_train)\n",
    "\n",
    "  # Score the classifier using the built in score function and the test set\n",
    "  # created earlier\n",
    "  score = knn_classifier.score(X_test, Y_test)\n",
    "\n",
    "  return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DsxVeY-QpR_j"
   },
   "source": [
    "Every time we run the score_model() function, a slightly different score is generated. To even out the final score a bit, we'll run it twenty times and return the average score (without the five lowest and highest values). This should help somewhat with grading the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dl3S_SVps_uN"
   },
   "outputs": [],
   "source": [
    "# Take the average of five runs of the score_model() function\n",
    "# and return a percentage\n",
    "def score_20(dataset):\n",
    "  scores = []\n",
    "  for i in range(20):\n",
    "    scores.append(score_model(dataset))\n",
    "\n",
    "  scores.sort()\n",
    "  average = sum(scores[5:15]) / 10\n",
    "  percentage = round(average * 100 , 2)\n",
    "\n",
    "  return percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yWcl1HNNTq9K"
   },
   "source": [
    "With the functions we need in place, let's get to scoring the actual datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 26307,
     "status": "ok",
     "timestamp": 1586380247890,
     "user": {
      "displayName": "Jasper Sijgers",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgMV5mgyq6OiBFr7_IG1qHskr3VehldL5sVO79W=s64",
      "userId": "09274429075529465880"
     },
     "user_tz": -120
    },
    "id": "i2iMWgzbprGd",
    "outputId": "a46659ab-9751-42d1-d145-6c3cd5459e4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid size: 1, average score: 11.35%\n",
      "grid size: 2, average score: 59.95%\n",
      "grid size: 4, average score: 94.58%\n",
      "grid size: 8, average score: 97.4%\n",
      "grid size: 16, average score: 97.71%\n"
     ]
    }
   ],
   "source": [
    "for i in [1, 2, 4, 8, 16, 32]:\n",
    "  dataset_path = DATASET_CSV_PATH + '/grid_dataset_' + str(i) + '.csv'\n",
    "  dataframe = pd.read_csv(dataset_path)\n",
    "\n",
    "  print('grid size: {0}, average score: {1}%'.format(i, score_20(dataframe)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pi85QXFnWQJ5"
   },
   "source": [
    "As the output above shows, and we could have predicted, simply counting the number of pixels in an image isn't going to be very accurate. Splitting it up into a 2x2 grid (4 squares) makes it a lot more accurate, but 60% still isn't good enough for our efforts. From a 4x4 grid and up, it becomes quite accurate: about 94% to 98%.\n",
    "\n",
    "There isn't much difference in the scores from grid size 8 to 32, although a grid size of 32 gives us quite a bit more squares than a grid size of 8. This could mean a lot more invaluable squares as well, which we can discard later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AYblfRMawHxd"
   },
   "source": [
    "## Automatic Feature Selection\n",
    "One way of selecting features is using the KBest function. We'll take the results we got earlier as a baseline, and see if we can improve it from there.\n",
    "\n",
    "```\n",
    "grid size: 8, average score: 97.45%\n",
    "grid size: 16, average score: 97.06%\n",
    "grid size: 32, average score: 97.08%\n",
    "```\n",
    "\n",
    "An 8x8 grid consists of 64 squares, meaning 64 values. A 16x16 grid consists of 256 values and a 32x32 grid of 1024 values.\n",
    "\n",
    "We're also going to need a slightly different scoring function, as we'll be splitting up the dataset into actual data and labels before we can select the features we want to keep. This means our scoring function will need to take two inputs instead of one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ehWeaUhUzLM2"
   },
   "outputs": [],
   "source": [
    "def score_model_2(X, Y):\n",
    "  # Create train and test sets (for both data and labels), with 90% of the data\n",
    "  # being used to train and 10% to test the model\n",
    "  X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.1)\n",
    "\n",
    "  # Create a KNN Classifier with K = 3\n",
    "  knn_classifier = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "  # Fit the classifier on the training set\n",
    "  knn_classifier.fit(X_train, Y_train)\n",
    "\n",
    "  # Score the classifier using the built in score function and the test set\n",
    "  # created earlier\n",
    "  score = knn_classifier.score(X_test, Y_test)\n",
    "\n",
    "  return score\n",
    "\n",
    "# Take the average of five runs of the score_model() function\n",
    "# and return a percentage\n",
    "def score_20_2(X, Y):\n",
    "  scores = []\n",
    "  for i in range(20):\n",
    "    scores.append(score_model_2(X, Y))\n",
    "\n",
    "  scores.sort()\n",
    "  average = sum(scores[5:15]) / 10\n",
    "  percentage = round(average * 100 , 2)\n",
    "\n",
    "  return percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YpPy-A8bbZtb"
   },
   "source": [
    "Now let's get to running some experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 62199,
     "status": "ok",
     "timestamp": 1586380421883,
     "user": {
      "displayName": "Jasper Sijgers",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgMV5mgyq6OiBFr7_IG1qHskr3VehldL5sVO79W=s64",
      "userId": "09274429075529465880"
     },
     "user_tz": -120
    },
    "id": "AFACqTVIXswB",
    "outputId": "0281edb9-64d5-4a05-e4eb-720a3c7434e8"
   },
   "outputs": [],
   "source": [
    "def get_K_best(grid_size, k_arr):\n",
    "  dataframe = pd.read_csv(DATASET_CSV_PATH + '/grid_dataset_' + str(grid_size) + '.csv')\n",
    "  \n",
    "  X = dataframe.drop('label', axis=1)\n",
    "  Y = dataframe['label']\n",
    "\n",
    "  scaler = MinMaxScaler()\n",
    "  X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "  for k in k_arr:\n",
    "    X_best = SelectKBest(chi2, k=k).fit_transform(X_scaled, Y)\n",
    "    print('grid size: {0}, K best: {1}, average score: {2}%'.format(grid_size, k, score_20_2(X_best, Y)))\n",
    "\n",
    "get_K_best(8, [16, 32, 48, 64])\n",
    "get_K_best(16, [64, 128, 192, 256])\n",
    "get_K_best(32, [128, 256, 512, 768, 1024])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XEdTbdfcpKxF"
   },
   "source": [
    "The output above shows no big difference in accuracy by removing some columns from the equation. It does show that if you remove a lot of columns, this negatively impacts the performance of the model.\n",
    "\n",
    "For a grid size of eight, a significant difference can be seen between 32 and 16 columns of data, with 16 having a 5% worse accuracy. For a grid size of eight a small difference can be seen at 64 in comparison to 128 columns and for a grid size of 32 this is true for 256 in comparison to 512.\n",
    "\n",
    "That said, no significant difference is noticable between a a grid size of eight (with anywhere from 32 to 64 columns), a grid size of 16 (with anywhere from 128 to 256 columns) or a grid size of 32 (with anywhere from 512 to 1024 columns).\n",
    "\n",
    "It would seem that our dataset is already quite optimized, and no further improvement can (easily) be obtained by selecting features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pF7eF1l4aiAp"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2jJKw25aajV7"
   },
   "source": [
    "# Image based model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NodY2-mBaqDl"
   },
   "source": [
    "Author: Dovydas Valiulis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bxCt8O4XcEE3"
   },
   "outputs": [],
   "source": [
    "# loading dataset that was created by feature extraction notebook\n",
    "df = pd.read_pickle(TEMP_DIR_PATH + \"/datasets/initial-image-feature-dataset.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5XqyWqC1b5ru"
   },
   "source": [
    "## Feature analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dddo8LMWcQ7b"
   },
   "source": [
    "In this part of the assignment, we will determine the best features to select for our classifier. We will look at statistics of the dataset to get an overall feeling of the data then we will graph boxplot for each feature grouped by a label to see if it is possible to separate labels from features. Furthermore, we will plot the histograms of each feature to see what is the distribution of each feature in our dataset. Then we will remove outliers and look at the graphs one more time. After that, we will look at the correlation graph to see which features are correlated with each other and could be removed from the final feature list. Finally, we will remove highly correlated features from our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "luu3d1BtccPr"
   },
   "source": [
    "The first step is to look at the statistics of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 836,
     "status": "ok",
     "timestamp": 1586548636499,
     "user": {
      "displayName": "Dovydas Valiulis",
      "photoUrl": "",
      "userId": "15268906399653486759"
     },
     "user_tz": -120
    },
    "id": "CXazodDnbx2q",
    "outputId": "72a03155-b96b-4fb3-9b98-d38cea897bfd"
   },
   "outputs": [],
   "source": [
    "# Showing statistics of the dataset\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ly_dOs6icmK6"
   },
   "source": [
    "From dataset statistics, we observed that there are obvious outliers in the dataset. For instance, some entries have an area of 1 and a perimeter of 0. These values can appear during the feature extraction phase when two or more regions are found in the single-digit image. To continue with our classification we will remove these outliers. To know how many outliers this dataset contains we draw boxplot and histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8086,
     "status": "ok",
     "timestamp": 1586548652975,
     "user": {
      "displayName": "Dovydas Valiulis",
      "photoUrl": "",
      "userId": "15268906399653486759"
     },
     "user_tz": -120
    },
    "id": "enix3PBdbx1C",
    "outputId": "bca1f83a-f4af-4997-85b6-ee8c9287dcdb"
   },
   "outputs": [],
   "source": [
    "# Drawing boxplots for each column grouped by label\n",
    "for column in df.columns:\n",
    "    df.boxplot([column], by='label',  figsize=(9, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PeHPq36ccpw7"
   },
   "source": [
    "From the boxplot, we observed that there are some outliers in each feature. Then we looked at the histogram graph to see where outliers are concentrated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5512,
     "status": "ok",
     "timestamp": 1586548661681,
     "user": {
      "displayName": "Dovydas Valiulis",
      "photoUrl": "",
      "userId": "15268906399653486759"
     },
     "user_tz": -120
    },
    "id": "KVPLHARGbxxO",
    "outputId": "b4c3325b-d7d9-43a6-c64d-21a633a9b423"
   },
   "outputs": [],
   "source": [
    "# drawing histograms for each column\n",
    "df.hist(figsize=(30, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IAYmkbKQculS"
   },
   "source": [
    "In histograms, we can see that most features have outliers. Features that have obvious outliers: \n",
    "1. Area\n",
    "2. Bbox area\n",
    "3. Convex area\n",
    "4. Eccentricity\n",
    "5. Equivalent diameter\n",
    "6. Extent\n",
    "7. Filled area\n",
    "8. Major axis length\n",
    "9. Perimeter\n",
    "10. Solidity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q4FXB3hMcwiy"
   },
   "source": [
    "Ten of our features have outliers. Also, we can see that most of the features might have the same outliers because the number of outliers is very similar from feature to feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y75kwsHYczYR"
   },
   "source": [
    "In this step, we will remove all entries where the area is less than 200 to remove outliers. we have chosen an area of 200 because if the area is less than 200 probably it is an extra region from the image feature extraction phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 908,
     "status": "ok",
     "timestamp": 1586548683517,
     "user": {
      "displayName": "Dovydas Valiulis",
      "photoUrl": "",
      "userId": "15268906399653486759"
     },
     "user_tz": -120
    },
    "id": "bC-VUDEUbxuK",
    "outputId": "7696988c-a004-455b-8f84-8598cb65ad19"
   },
   "outputs": [],
   "source": [
    "# removing outliers\n",
    "df = df[df['area'] > 200]\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Eu_RU5tuc3Uc"
   },
   "source": [
    "Removing outliers based on one feature also removed outliers from other features. We can already see that minimum values make much more sense. There are no obvious outliers that we can see from dataset statistics. in order to confirm that we plotted boxplots and histograms again to see how these graphs have changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7788,
     "status": "ok",
     "timestamp": 1586548696796,
     "user": {
      "displayName": "Dovydas Valiulis",
      "photoUrl": "",
      "userId": "15268906399653486759"
     },
     "user_tz": -120
    },
    "id": "omfYgF15bxqr",
    "outputId": "49b5ad46-3532-418e-d644-d36eddd0a413"
   },
   "outputs": [],
   "source": [
    "# Checkong how boxplot graphs changed after removal of outliers\n",
    "for column in df.columns:\n",
    "    df.boxplot([column], by='label',  figsize=(9, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5914,
     "status": "ok",
     "timestamp": 1586548705234,
     "user": {
      "displayName": "Dovydas Valiulis",
      "photoUrl": "",
      "userId": "15268906399653486759"
     },
     "user_tz": -120
    },
    "id": "nxKI7HYKbxou",
    "outputId": "c6644aa4-bab4-4a53-ac5a-eccd38aeb44b"
   },
   "outputs": [],
   "source": [
    "# Checkong how histogram graphs changed after removal of outliers\n",
    "df.hist(figsize=(30, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aRxl4NVRc7yi"
   },
   "source": [
    "As we can see from the plots we have a lot fewer outliers. Furthermore, we can see that it should be possible to differentiate amongst different labels using a combination of different features. Also, it is clear which features do not offer any meaningful information for classification. these features are equivalent diameter, bbox-0, and bbox-2. Other features should offer some useful insight for the classification algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lF-qNglWc7pz"
   },
   "source": [
    "Next, we looked at the correlation matrix in order to see which features are highly correlated and remove them. There is no point having features that are highly correlated because they will have no or very little impact on the classification result but it will have an impact on processing speed. We have decided to remove all features over 80% correlation or below 80 % correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1057,
     "status": "ok",
     "timestamp": 1586548709677,
     "user": {
      "displayName": "Dovydas Valiulis",
      "photoUrl": "",
      "userId": "15268906399653486759"
     },
     "user_tz": -120
    },
    "id": "qNq5nwVAbxk6",
    "outputId": "6c4a2d65-16d6-4b73-eb65-59a90698c686"
   },
   "outputs": [],
   "source": [
    "# Checkong how corrolation changed after removal of outliers\n",
    "corr = df.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm', axis=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OmkQnVTxdEVF"
   },
   "source": [
    "## Export labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ekwUvD_0dMeJ"
   },
   "source": [
    "Now that we have a complete dataset where outliers have been removed we exported labels of the dataset to the separate .pkl file. this file will be loaded in the next phase and will be used for training, testing, and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 845,
     "status": "ok",
     "timestamp": 1586548783866,
     "user": {
      "displayName": "Dovydas Valiulis",
      "photoUrl": "",
      "userId": "15268906399653486759"
     },
     "user_tz": -120
    },
    "id": "M5dLdbmQbxhN",
    "outputId": "48deff3c-1eca-4dfd-fffe-26961e9fb247"
   },
   "outputs": [],
   "source": [
    "# Exporting dataset labels. To be used in training and testing later on\n",
    "labels = df['label'].to_numpy()\n",
    "joblib.dump(labels, TEMP_DIR_PATH + '/datasets/label-dataset.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qH407WLVdgVP"
   },
   "source": [
    "## Manual feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BauskWtedgHk"
   },
   "source": [
    "Based on the correlation matrix and histograms we have decided to remove these features: \n",
    "  1. equivalent diameter - due to high correlation with area feature\n",
    "  2. bbox-0 - due to the very small distribution observed in histogram\n",
    "  3. bbox-2 - due to the very small distribution observed in histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wprtxGZ0bxcJ"
   },
   "outputs": [],
   "source": [
    "# Manual feature removal\n",
    "df_mnl = df.drop(['equivalent_diameter', 'bbox-0', 'bbox-2'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l-1ITWr0dr9J"
   },
   "source": [
    "## Manual feature pre-processing \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P0ivFfV5drdZ"
   },
   "source": [
    "To classify entries of the dataset features must be scaled. This is important to have all features that have the same weight in most classification algorithms. We have selected to use Normalization (MinMaxScaler)\n",
    "instead of Standardization because we already removed obvious outliers and we wanted to maintain our scale between entries. Furthermore, most of our features already are normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5lTVLkrUbxZO"
   },
   "outputs": [],
   "source": [
    "# Scale manualy selected features\n",
    "X_mnl_df = df_mnl.iloc[:, df_mnl.columns != 'label'].reindex()\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_mnl_scld = scaler.fit_transform(X_mnl_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TduiD_SXdyY-"
   },
   "source": [
    "After we have scaled our features we export them to the .pkl file alongside the scaler that was used. Scaler export needed for scaling new images to use in classifier after it was trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 777,
     "status": "ok",
     "timestamp": 1586549084780,
     "user": {
      "displayName": "Dovydas Valiulis",
      "photoUrl": "",
      "userId": "15268906399653486759"
     },
     "user_tz": -120
    },
    "id": "4pV318vQbxXr",
    "outputId": "e62ce5b3-47a7-4f37-d9da-aacaf3eb13d2"
   },
   "outputs": [],
   "source": [
    "# Export dataset and scaler. Dataset will be used in training and testing in the next step\n",
    "if not os.path.isdir(TEMP_DIR_PATH + \"/datasets\"):\n",
    "  os.mkdir(TEMP_DIR_PATH + \"/datasets\")\n",
    "\n",
    "if not os.path.isdir(TEMP_DIR_PATH + \"/preprocessors\"):\n",
    "  os.mkdir(TEMP_DIR_PATH + \"/preprocessors\")\n",
    "\n",
    "joblib.dump(X_mnl_scld, TEMP_DIR_PATH + '/datasets/manualy-selected-feature-dataset.pkl')\n",
    "joblib.dump(scaler, TEMP_DIR_PATH + '/preprocessors/manual-image-feature-preprocessor.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0FXLyFZseOzq"
   },
   "source": [
    "## Auto feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aqKBtpNjeW15"
   },
   "source": [
    "Also, we have decided to use auto feature selection sklearn to see if automatically selected features would yield better results. The first step to auto feature selection is the scaling dataset. Same as for manual feature selection we are using MinMaxScaler. After that, we are using the SelectKBest module from the sklearn. This module has two hyperparameters: score_func and k.\n",
    "score_func is a method used for scoring each feature how useful it is and k is the number of features we want to be remaining in the final dataset. For our score_func we have selected to use \"chi2\" and \"f_classif\" because these functions are used to select the best features for classification task. For our k we decided to use 16, to make our manually selected feature amount and automatically selected feature amount be the same. In the next phase, we will test both datasets from \"chi2\" and \"f_classif\" and see which is better in for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Fd6yKwxeOj3"
   },
   "outputs": [],
   "source": [
    "# Scale features\n",
    "X_auto_df = df.iloc[:, df.columns != 'label']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_auto_scld = scaler.fit_transform(X_auto_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZEWgNf-KbxTi"
   },
   "outputs": [],
   "source": [
    "# select best features out of the dataset\n",
    "skb_chi2 = SelectKBest(chi2, k=16)\n",
    "X_auto_chi2 = skb_chi2.fit_transform(X_auto_scld, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6YdCTVdKbxCg"
   },
   "outputs": [],
   "source": [
    "skb_f_classif = SelectKBest(f_classif, k=16)\n",
    "X_auto_f_classif = skb_f_classif.fit_transform(X_auto_scld, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PENMgHjtecA6"
   },
   "source": [
    "Same as with manual features we need to export selected features and scalers. Besides them, we need to export two SelectKBest modules that were already fitted. This is required in order to extract the best features from new images that will be uploaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 888,
     "status": "ok",
     "timestamp": 1586549059646,
     "user": {
      "displayName": "Dovydas Valiulis",
      "photoUrl": "",
      "userId": "15268906399653486759"
     },
     "user_tz": -120
    },
    "id": "5MB7GsLRbw_J",
    "outputId": "1942bcae-22d9-4d22-d244-1e7018eb2d0b"
   },
   "outputs": [],
   "source": [
    "joblib.dump(scaler, TEMP_DIR_PATH + '/preprocessors/auto-image-feature-preprocessor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MEpVXxZzrjzo"
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir(TEMP_DIR_PATH + \"/selection\"):\n",
    "  os.mkdir(TEMP_DIR_PATH + \"/selection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 785,
     "status": "ok",
     "timestamp": 1586549065975,
     "user": {
      "displayName": "Dovydas Valiulis",
      "photoUrl": "",
      "userId": "15268906399653486759"
     },
     "user_tz": -120
    },
    "id": "M3hJE-bHbw9C",
    "outputId": "bf33543f-1783-443a-8497-2e3079d75a0b"
   },
   "outputs": [],
   "source": [
    "# Export dataset, scaler and feature selector. Dataset will be used in training and testing in the next step\n",
    "joblib.dump(X_auto_chi2, TEMP_DIR_PATH + '/datasets/auto-selected-features-chi2.pkl')\n",
    "joblib.dump(skb_chi2, TEMP_DIR_PATH + '/selection/auto-image-feature-selector-chi2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 869,
     "status": "ok",
     "timestamp": 1586549068751,
     "user": {
      "displayName": "Dovydas Valiulis",
      "photoUrl": "",
      "userId": "15268906399653486759"
     },
     "user_tz": -120
    },
    "id": "RH4L1_E3bw5Y",
    "outputId": "13b3b6b0-bbb5-40bd-fdb1-acf168208a3a"
   },
   "outputs": [],
   "source": [
    "# Export dataset, scaler and feature selector. Dataset will be used in training and testing in the next step\n",
    "joblib.dump(X_auto_f_classif, TEMP_DIR_PATH + '/datasets/auto-selected-features-f-classif.pkl')\n",
    "joblib.dump(skb_f_classif, TEMP_DIR_PATH + '/selection/auto-image-feature-selector-f-classif.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i-IgLHzE85no"
   },
   "source": [
    "# Cleanup\n",
    "Ensure no data is left on the runtime after execution of all code has completed. This ensures we won't re-use old data once something in the code has changed, eliminating the risk of hours of debugging functional code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 512,
     "status": "ok",
     "timestamp": 1586380860502,
     "user": {
      "displayName": "Jasper Sijgers",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgMV5mgyq6OiBFr7_IG1qHskr3VehldL5sVO79W=s64",
      "userId": "09274429075529465880"
     },
     "user_tz": -120
    },
    "id": "1b3XVy4t8Ey0",
    "outputId": "6e6de8f7-fb31-4d44-f1d5-7c7bb694055a"
   },
   "outputs": [],
   "source": [
    "# # Remove all data from the /content directory\n",
    "# if os.path.isdir(DATASET_CSV_PATH):\n",
    "#   shutil.rmtree(DATASET_CSV_PATH)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Feature Analysis",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
